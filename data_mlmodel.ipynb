{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "b3ba2566441a7c06988d0923437866b63cedc61552a5af99d1f4fb67d367b25f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices1621 = pd.read_csv('Data_temp/prices1621.csv')\n",
    "prices1621.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = prices1621[['town', 'flat_type', 'storey_range', 'floor_area_sqm', 'flat_model', 'lease_commence_date', 'year', 'school_dist', 'num_school_2km', 'hawker_dist', 'num_hawker_2km', 'park_dist', 'num_park_2km', 'mall_dist', 'num_mall_2km', 'mrt_dist', 'num_mrt_2km', 'supermarket_dist', 'num_supermarket_2km', 'dist_cityhall', 'region', 'real_price']]\n",
    "\n",
    "# function for replacing NAs with median of the town\n",
    "def replace_NA_median(df, columns):\n",
    "    for c in columns:      \n",
    "        df[c] = df.groupby(\"town\").transform(lambda x: x.fillna(x.median()))[c]\n",
    "    return df\n",
    "\n",
    "df = replace_NA_median(df, ['school_dist', 'num_school_2km', 'hawker_dist',\n",
    "       'num_hawker_2km', 'park_dist', 'num_park_2km', 'mall_dist',\n",
    "       'num_mall_2km', 'mrt_dist', 'num_mrt_2km', 'supermarket_dist',\n",
    "       'num_supermarket_2km', 'dist_cityhall'])\n",
    "df.info()"
   ]
  },
  {
   "source": [
    "# Multicollinearity"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation heatmap\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = sns.heatmap(df.select_dtypes(include=['int64','float64']).corr(), annot = True, fmt='.2g', \n",
    "    vmin=-1, vmax=1, center= 0, cmap= 'coolwarm_r', linecolor='black', linewidth=1, annot_kws={\"size\": 7})\n",
    "#ax.set_ylim(0 ,5)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title('Correlation Heatmap')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multicollinearity\n",
    "# Import library for VIF\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "def calc_vif(X):\n",
    "    # Calculating VIF\n",
    "    vif = pd.DataFrame()\n",
    "    vif[\"variables\"] = X.columns\n",
    "    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n",
    "    vif['tolerance'] = 1/vif.VIF\n",
    "    vif['meanVIF'] = vif.VIF.mean()\n",
    "\n",
    "    return(vif)\n",
    "\n",
    "calc_vif(df.drop('real_price',axis=1).select_dtypes(include=['int64','float64']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_vif_table = calc_vif(df.drop(['real_price','num_supermarket_2km','year','num_school_2km','dist_cityhall'],axis=1).select_dtypes(include=['int64','float64']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_vif_table_sorted = calc_vif_table.sort_values(by=['VIF'], ascending=False)\n",
    "calc_vif_table_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop columns\n",
    "lr_df = df.drop(['num_supermarket_2km','year','num_school_2km','dist_cityhall'], axis=1)\n",
    "lr_df.head()"
   ]
  },
  {
   "source": [
    "# Normality"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distribution for each continuous variable\n",
    "lr_df.hist(bins=50, figsize=(15, 10), grid=False, edgecolor='black')\n",
    "plt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot qqplot before and after log transformation\n",
    "\n",
    "from statsmodels.api import qqplot\n",
    "fig, ((ax1,ax2), (ax3,ax4)) = plt.subplots(2,2,figsize=(5,5))\n",
    "\n",
    "ax1.hist(lr_df['real_price'], bins=50, edgecolor='black')\n",
    "qqplot(lr_df['real_price'], line='s', ax=ax2)\n",
    "ax3.hist(np.log(lr_df['real_price']), bins=50, edgecolor='black')\n",
    "qqplot(np.log(lr_df['real_price']), line='s', ax=ax4)\n",
    "plt.suptitle('Real Price Normality Before (Top) & After (Bottom) Logging')\n",
    "plt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Label & Dummy Encoding"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frequency plots for catergorical features\n",
    "fig = plt.figure(figsize=(30,5))\n",
    "for count, col in enumerate(df.select_dtypes(include=['category','object']).columns):\n",
    "    fig.add_subplot(1,5,count+1)\n",
    "    df[col].value_counts().plot.barh()\n",
    "    plt.title(col)\n",
    "    plt.yticks(rotation=45)\n",
    "    plt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encode storeys\n",
    "df = df.sort_values(by='storey_range')\n",
    "df['storey_range'] = df['storey_range'].astype('category').cat.codes # label encode\n",
    "lr_df = lr_df.sort_values(by='storey_range')\n",
    "lr_df['storey_range'] = lr_df['storey_range'].astype('category').cat.codes # label encode\n",
    "\n",
    "# remove flat types with very few cases\n",
    "df = df[~df['flat_type'].isin(['MULTI GENERATION', '1 ROOM'])]\n",
    "lr_df = lr_df[~lr_df['flat_type'].isin(['MULTI GENERATION', '1 ROOM'])]\n",
    "\n",
    "# Re-categorize flat model to reduce num classes\n",
    "replace_values = {'Executive Maisonette':'Maisonette', 'Terrace':'Special', 'Adjoined flat':'Special', \n",
    "                    'Type S1S2':'Special', 'DBSS':'Special', 'Model A2':'Model A', 'Premium Apartment':'Apartment', 'Improved':'Standard', 'Simplified':'Model A', '2-room':'Standard'}\n",
    "df = df.replace({'flat_model': replace_values})\n",
    "lr_df = lr_df.replace({'flat_model': replace_values})\n",
    "\n",
    "# Label encode flat type\n",
    "replace_values = {'2 ROOM':0, '3 ROOM':1, '4 ROOM':2, '5 ROOM':3, 'EXECUTIVE':4}\n",
    "df = df.replace({'flat_type': replace_values})\n",
    "lr_df = lr_df.replace({'flat_type': replace_values})\n",
    "\n",
    "df = df.reset_index(drop=True)\n",
    "display(df['flat_model'].value_counts())\n",
    "lr_df = lr_df.reset_index(drop=True)\n",
    "display(lr_df['flat_model'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## dummy encoding\n",
    "df = pd.get_dummies(df, columns=['region'], prefix=['region'], drop_first=True) # central is baseline\n",
    "df = pd.get_dummies(df, columns=['flat_model'], prefix=['model'])\n",
    "df= df.drop('model_Standard',axis=1) # remove standard, setting it as the baseline\n",
    "lr_df = pd.get_dummies(lr_df, columns=['region'], prefix=['region'], drop_first=True) # central is baseline\n",
    "lr_df = pd.get_dummies(lr_df, columns=['flat_model'], prefix=['model'])\n",
    "lr_df= lr_df.drop('model_Standard',axis=1) # remove standard, setting it as the baseline"
   ]
  },
  {
   "source": [
    "# Feature Scaling"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# fit to continuous columns and transform\n",
    "scaled_columns = ['floor_area_sqm','lease_commence_date','school_dist','hawker_dist','num_hawker_2km','park_dist',\n",
    "                    'num_park_2km', 'mall_dist', 'num_mall_2km', 'mrt_dist', 'num_mrt_2km', 'supermarket_dist']\n",
    "scaler.fit(lr_df[scaled_columns])\n",
    "scaled_columns = pd.DataFrame(scaler.transform(lr_df[scaled_columns]), index=lr_df.index, columns=scaled_columns)\n",
    "\n",
    "# separate unscaled features\n",
    "unscaled_columns = lr_df.drop(scaled_columns, axis=1)\n",
    "\n",
    "# concatenate scaled and unscaled features\n",
    "lr_df = pd.concat([scaled_columns,unscaled_columns], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_value = float(\"NaN\")\n",
    "lr_df.replace(\"\", nan_value, inplace=True)\n",
    "lr_df.dropna(subset = [\"real_price\"], inplace=True)\n",
    "# len(lr_df)"
   ]
  },
  {
   "source": [
    "# Outlier Detection"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import CooksDistance\n",
    "\n",
    "lr_y = lr_df[['real_price']]\n",
    "lr_X = lr_df.drop(['real_price','town'], axis=1)\n",
    "\n",
    "yy = np.log(lr_y)['real_price']\n",
    "XX = lr_X.values\n",
    "\n",
    "visualizer = CooksDistance()\n",
    "visualizer.fit(XX, yy)\n",
    "visualizer.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# visualize residuals before outlier removal\n",
    "model = LinearRegression()\n",
    "visualizer_residuals = ResidualsPlot(model)\n",
    "visualizer_residuals.fit(XX, yy)\n",
    "visualizer_residuals.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers\n",
    "i_less_influential = (visualizer.distance_ <= visualizer.influence_threshold_)\n",
    "X_li, y_li = XX[i_less_influential], yy[i_less_influential]\n",
    "lr_X, lr_y = lr_X[i_less_influential], lr_y[i_less_influential]\n",
    "\n",
    "# visualize residuals after outliers removal\n",
    "model = LinearRegression()\n",
    "visualizer_residuals = ResidualsPlot(model)\n",
    "visualizer_residuals.fit(X_li, y_li)\n",
    "visualizer_residuals.show()\n",
    "# plt.close()"
   ]
  },
  {
   "source": [
    "# Linear Regression"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# sklearn method, which doesn't give much additional info\n",
    "\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(lr_X, np.log(lr_y))\n",
    "\n",
    "print(f'Coefficients: {lin_reg.coef_}')\n",
    "print(f'Intercept: {lin_reg.intercept_}')\n",
    "print(f'R^2 score: {lin_reg.score(lr_X, np.log(lr_y))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# statsmodel method, which gives more info\n",
    "import statsmodels.api as sm\n",
    "# alternate way using statistical formula, which does not require dummy coding manually\n",
    "# https://stackoverflow.com/questions/50733014/linear-regression-with-dummy-categorical-variables\n",
    "# https://stackoverflow.com/questions/34007308/linear-regression-analysis-with-string-categorical-features-variables\n",
    "\n",
    "X_constant = sm.add_constant(lr_X)\n",
    "lin_reg = sm.OLS(np.log(lr_y),X_constant).fit()\n",
    "lin_reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplot of y (observed) and yhat (predicted)\n",
    "\n",
    "plt.style.use('default')\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "ax = sns.scatterplot(x=np.log(lr_y)['real_price'], y=lin_reg.predict(), edgecolors='w', alpha=0.9, s=8)\n",
    "ax.set_xlabel('Observed')#, ax.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax.get_xticks()/1000])\n",
    "ax.set_ylabel('Predicted')#, ax.set_yticklabels(['{:,.0f}'.format(x) + 'K' for x in ax.get_yticks()/1000])\n",
    "ax.annotate('Adjusted R\\u00b2: ' + str(format(round(lin_reg.rsquared_adj,2),'.2f')), xy=(0, 1), xytext=(25, -25),\n",
    "    xycoords='axes fraction', textcoords='offset points', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Homoscedasticity and Normality of Residuals"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Homoscedasticity and Normality of Residuals\n",
    "pred = lin_reg.predict()\n",
    "resids = lin_reg.resid\n",
    "resids_studentized = lin_reg.get_influence().resid_studentized_internal\n",
    "\n",
    "fig = plt.figure(figsize=(10,3))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "sns.scatterplot(x=pred, y=resids_studentized, edgecolors='w', alpha=0.9, s=8)\n",
    "ax1.set_xlabel('Predicted Values')\n",
    "ax1.set_ylabel('Studentized Residuals')\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "sns.distplot(resids_studentized, norm_hist=True, hist_kws=dict(edgecolor='w'))\n",
    "ax2.set_xlabel('Studentized Residual')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance\n",
    "\n",
    "lr_results = pd.read_html(lin_reg.summary().tables[1].as_html(),header=0,index_col=0)[0]\n",
    "coefs = lr_results[['coef']][1:].reset_index().rename(columns={'index':'feature'})\n",
    "coefs['feature_importance'] = np.abs(coefs['coef'])\n",
    "coefs = coefs.sort_values('feature_importance').reset_index(drop=True)\n",
    "coefs['color'] = coefs['coef'].apply(lambda x: '#66ff8c' if x>0 else '#ff8c66')\n",
    "coefs.plot.barh(x='feature',y='feature_importance',color=coefs['color'],figsize=(4,5))\n",
    "colors = {'Positive':'#66ff8c', 'Negative':'#ff8c66'}         \n",
    "labels = list(colors.keys())\n",
    "handles = [plt.Rectangle((0,0),1,1, color=colors[label]) for label in labels]\n",
    "legend = plt.legend(handles, labels, title='Relationship', fontsize = '15')\n",
    "plt.setp(legend.get_title(),fontsize='17')\n",
    "plt.xlabel('Standardized Coefficients', size=15), plt.ylabel('Features', size=15)\n",
    "plt.ylim([-1,23])\n",
    "plt.title('Linear Regression Feature Importance', size=15)\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Random Forest"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train Test Split\n",
    "y = df[['real_price']]\n",
    "X = df.drop(['real_price','town', 'year'], axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.1, shuffle=True, random_state=0)\n",
    "print('Shape of X_train:', X_train.shape)\n",
    "print('Shape of X_test:', X_test.shape)\n",
    "print('Shape of y_train:', y_train.shape)\n",
    "print('Shape of y_test:', y_test.shape)"
   ]
  },
  {
   "source": [
    "# Out-of-Bag"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create our imputer to replace missing values with the mean e.g.\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp_test = imp.fit(y_test)\n",
    "imp_train = imp.fit(y_train)\n",
    "\n",
    "# Validation using out-of-bag method\n",
    "y_train_imp = imp_train.transform(y_train)\n",
    "y_test_imp = imp_test.transform(y_test)\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, oob_score=True, random_state=0)\n",
    "rf.fit(X_train, y_train_imp.ravel())\n",
    "predicted_train = rf.predict(X_train)\n",
    "\n",
    "print(f'Out-of-bag R\\u00b2 score estimate: {rf.oob_score_:>5.3}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and get evaluation metrics on test set\n",
    "y_test2 = y_test.apply(lambda x: x.fillna(x.mean()),axis=0)\n",
    "\n",
    "predicted_test = rf.predict(X_test)\n",
    "oob_test_score = r2_score(y_test2['real_price'], predicted_test)\n",
    "spearman = spearmanr(y_test2['real_price'], predicted_test)\n",
    "pearson = pearsonr(y_test2['real_price'], predicted_test)\n",
    "oob_mae = mean_absolute_error(y_test2['real_price'], predicted_test)\n",
    "\n",
    "print(f'Test data R\\u00b2 score: {oob_test_score:>5.3}')\n",
    "print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
    "print(f'Test data Pearson correlation: {pearson[0]:.3}')\n",
    "print(f'Test data Mean Absolute Error: {round(oob_mae)}')"
   ]
  },
  {
   "source": [
    "# K-fold Cross Validation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# validation by k-fold cross validation with grid search for best hyperparameters\n",
    "# hyperparameter values shown below are the tuned final values\n",
    "param_grid = {\n",
    "    'max_features': ['auto'], # max number of features considered for splitting a node\n",
    "    'max_depth': [20], # max number of levels in each decision tree\n",
    "    'min_samples_split': [15], # min number of data points placed in a node before the node is split\n",
    "    'min_samples_leaf': [2]} # min number of data points allowed in a leaf node\n",
    "rfr =GridSearchCV(RandomForestRegressor(n_estimators = 500, n_jobs=-1, random_state=0),\n",
    "                        param_grid, cv=10, scoring='r2', return_train_score=True)\n",
    "rfr.fit(X_train,y_train_imp.ravel())\n",
    "print(\"Best parameters set found on Cross Validation:\\n\\n\", rfr.best_params_)\n",
    "print(\"\\nCross Validation R\\u00b2 score:\\n\\n\", rfr.best_score_.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and get evaluation metrics for test set\n",
    "\n",
    "cv_predicted_test = rfr.predict(X_test)\n",
    "\n",
    "cv_test_score = r2_score(y_test2['real_price'], cv_predicted_test)\n",
    "spearman = spearmanr(y_test2['real_price'], cv_predicted_test)\n",
    "pearson = pearsonr(y_test2['real_price'], cv_predicted_test)\n",
    "cv_mae = mean_absolute_error(y_test2['real_price'], cv_predicted_test)\n",
    "\n",
    "print(f'Test data R\\u00b2 score: {cv_test_score:>5.3}')\n",
    "print(f'Test data Spearman correlation: {spearman[0]:.3}')\n",
    "print(f'Test data Pearson correlation: {pearson[0]:.3}')\n",
    "print(f'Test data Mean Absolute Error: {round(cv_mae)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scatterplots of y (observed) and yhat (predicted)\n",
    "\n",
    "fig = plt.figure(figsize=(13,4))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "ax1 = sns.scatterplot(x=y_test['real_price'], y=predicted_test, edgecolors='w', alpha=0.9, s=8)\n",
    "ax1.set_xlabel('Observed'), ax1.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax1.get_xticks()/1000])\n",
    "ax1.set_ylabel('Predicted'), ax1.set_yticklabels(['{:,.0f}'.format(x) + 'K' for x in ax1.get_yticks()/1000])\n",
    "ax1.annotate('Test R\\u00b2: ' + str(round(oob_test_score,3)) + '\\nTest MAE: ' + str(round(oob_mae)), xy=(0, 1), xytext=(25, -35),\n",
    "    xycoords='axes fraction', textcoords='offset points', fontsize=12)\n",
    "ax1.set_title('Tuned Using Out-Of-Bag')\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "ax2 = sns.scatterplot(x=y_test['real_price'], y=cv_predicted_test, edgecolors='w', alpha=0.9, s=8)\n",
    "ax2.set_xlabel('Observed'), ax2.set_xticklabels(['{:,.0f}'.format(x) + 'K' for x in ax2.get_xticks()/1000])\n",
    "ax2.set_ylabel('Predicted'), ax2.set_yticklabels(['{:,.0f}'.format(x) + 'K' for x in ax2.get_yticks()/1000])\n",
    "ax2.annotate('Test R\\u00b2: ' + str(round(cv_test_score,3)) + '\\nTest MAE: ' + str(round(cv_mae)), xy=(0, 1), xytext=(25, -35),\n",
    "    xycoords='axes fraction', textcoords='offset points', fontsize=12)\n",
    "ax2.set_title('Tuned Using Cross Validation')\n",
    "plt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\n",
    "plt.show()"
   ]
  },
  {
   "source": [
    "# Feature Importance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,7))\n",
    "\n",
    "ax1 = plt.subplot(121)\n",
    "feat_imp = pd.DataFrame({'Features': X_train.columns, 'Feature Importance': rf.feature_importances_}).sort_values('Feature Importance', ascending=False)\n",
    "sns.barplot(y='Features', x='Feature Importance', data=feat_imp)\n",
    "#plt.xticks(rotation=45, ha='right')\n",
    "ax1.set_title('OOB Feature Importance', size=15)\n",
    "\n",
    "ax2 = plt.subplot(122)\n",
    "feat_imp = pd.DataFrame({'Features': X_train.columns, 'Feature Importance': rfr.best_estimator_.feature_importances_}).sort_values('Feature Importance', ascending=False)\n",
    "sns.barplot(y='Features', x='Feature Importance', data=feat_imp)\n",
    "ax2.set_title('CV Feature Importance', size=15)\n",
    "\n",
    "plt.tight_layout(pad=0, rect=[0, 0, 0.9, 0.9])\n",
    "fig.show()"
   ]
  },
  {
   "source": [
    "# SHAP Values"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flats with predicted low resale price\n",
    "\n",
    "import shap\n",
    "shap.initjs()\n",
    "\n",
    "explainer = shap.TreeExplainer(rfr.best_estimator_)\n",
    "shap_values = explainer.shap_values(X_test.iloc[[16]])\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[[16]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model\n",
    "\n",
    "import joblib\n",
    "\n",
    "joblib.dump(rfr.best_estimator_, 'rf_compressed.pkl', compress=3) # smaller size\n",
    "joblib.dump(explainer, 'shap_explainer.pkl', compress=3)"
   ]
  }
 ]
}